{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to `Scikit-Learn`\n",
    "\n",
    "#### Scikit-learn is a powerful Python library for machine learning. It provides simple and efficient tools for data mining and data analysis. It supports various machine learning algorithms, which can be applied for classification, regression, clustering, and dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Data\n",
    "\n",
    "#### Before applying any machine learning algorithms, you need data. Let's use dummy data to make this easier to follow.\n",
    "\n",
    "##### `Scenario`: You are working on predicting whether a customer will buy a product based on their features like age, income, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in e:\\codes\\.venv\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in e:\\codes\\.venv\\lib\\site-packages (from scikit-learn) (2.0.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in e:\\codes\\.venv\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\codes\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\codes\\.venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  Income\n",
      "8   62   64000\n",
      "1   25   18000\n",
      "5   56   55000\n",
      "8    1\n",
      "1    0\n",
      "5    1\n",
      "Name: Buy, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Creating a dummy dataset\n",
    "data = {\n",
    "    'Age': [22, 25, 47, 52, 46, 56, 56, 60, 62, 61],\n",
    "    'Income': [15000, 18000, 32000, 40000, 52000, 55000, 60000, 62000, 64000, 65000],\n",
    "    'Buy': [0, 0, 1, 1, 1, 1, 1, 1, 1, 1]  # 0 means No, 1 means Yes\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target\n",
    "X = df[['Age', 'Income']]  # Features\n",
    "y = df['Buy']  # Target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(X_test)\n",
    "print(y_test)\n",
    "\n",
    "\n",
    "# Explanation:\n",
    "\n",
    "# train_test_split is used to divide data into training and testing sets.\n",
    "# The features (Age and Income) are independent variables, \n",
    "# while Buy is the dependent variable (whether a customer buys the product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing Data\n",
    "#### Scikit-learn provides tools for preprocessing data. Sometimes, we need to scale or normalize data to ensure that machine learning algorithms perform well.\n",
    "\n",
    "##### `Scenario`: Age and income are on different scales (e.g., 20-70 for age, 10k-100k for income). Machine learning algorithms might perform poorly with unscaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Income</th>\n",
       "      <th>Buy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>15000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>18000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>32000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>40000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>52000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>55000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>60000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60</td>\n",
       "      <td>62000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>62</td>\n",
       "      <td>64000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>61</td>\n",
       "      <td>65000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Income  Buy\n",
       "0   22   15000    0\n",
       "1   25   18000    0\n",
       "2   47   32000    1\n",
       "3   52   40000    1\n",
       "4   46   52000    1\n",
       "5   56   55000    1\n",
       "6   56   60000    1\n",
       "7   60   62000    1\n",
       "8   62   64000    1\n",
       "9   61   65000    1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.20069019 -1.8495181 ]\n",
      " [ 0.88027607  0.9038369 ]\n",
      " [-0.1737387  -0.85362374]\n",
      " [ 0.96135413  1.07958296]\n",
      " [-0.25481676  0.31801669]\n",
      " [ 0.2316516  -0.38496757]\n",
      " [ 0.55596384  0.78667286]]\n",
      "[[0.         0.        ]\n",
      " [0.97435897 0.94      ]\n",
      " [0.64102564 0.34      ]\n",
      " [1.         1.        ]\n",
      " [0.61538462 0.74      ]\n",
      " [0.76923077 0.5       ]\n",
      " [0.87179487 0.9       ]]\n"
     ]
    }
   ],
   "source": [
    "## StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "# Standardizing data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(X_train_scaled)\n",
    "\n",
    "\n",
    "## Explanation:\n",
    "\n",
    "# StandardScaler scales the data such that it has a mean of 0 and a standard deviation of 1. \n",
    "# This is useful when working with algorithms that require normally distributed data (like SVM or Logistic Regression).\n",
    "\n",
    "## MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#print(X_train_scaled)\n",
    "print(X_test_scaled)\n",
    "\n",
    "\n",
    "# Description: Scales features to a given range, usually [0, 1].\n",
    "# Use Case: Useful when the bounds of the data are known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classification using Logistic Regression\n",
    "\n",
    "#### Logistic Regression is used when the target variable is binary (like \"Buy\" or \"Not Buy\").\n",
    "\n",
    "##### `Scenario`: Predicting whether a customer will buy a product based on age and income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values: [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create and train the model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "print(\"Predicted values:\", y_pred)\n",
    "\n",
    "\n",
    "# Explanation:\n",
    "\n",
    "# Logistic Regression is a classification algorithm \n",
    "# used when the dependent variable is binary.\n",
    "# After training, you can use the predict method to classify unseen data.\n",
    "\n",
    "## Email Data\n",
    "## Its a spam or not!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluating the Model\n",
    "#### After training the model, it's important to evaluate its performance using metrics like accuracy, precision, recall, and F1 score.\n",
    "\n",
    "##### `Scenario`: You want to know how well the model is predicting customer behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Confusion Matrix**\n",
    "A confusion matrix is a summary of prediction results for a classification problem. It shows the number of correct and incorrect predictions categorized by actual classes and predicted classes.\n",
    "\n",
    "The confusion matrix looks like this:\n",
    "- **True Positives (TP)**: Correctly predicted positive class (actual = 1, predicted = 1).\n",
    "- **True Negatives (TN)**: Correctly predicted negative class (actual = 0, predicted = 0).\n",
    "- **False Positives (FP)**: Incorrectly predicted positive class (actual = 0, predicted = 1).\n",
    "- **False Negatives (FN)**: Incorrectly predicted negative class (actual = 1, predicted = 0).\n",
    "\n",
    "|               | Predicted: No (0) | Predicted: Yes (1) |\n",
    "|---------------|------------------|-------------------|\n",
    "| **Actual: No (0)** | True Negative (TN) | False Positive (FP) |\n",
    "| **Actual: Yes (1)** | False Negative (FN) | True Positive (TP) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[0 1]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming `y_test` is the actual labels and `y_pred` is the predicted labels\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "The confusion matrix gives us an understanding of the model’s classification performance on each category.\n",
    "\n",
    "For example:\n",
    "- If the model is classifying too many \"No\" customers as \"Yes,\" that would reflect as a higher False Positive (FP) count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Accuracy**\n",
    "Accuracy is simply the ratio of correctly predicted instances to the total number of instances. However, accuracy can be misleading in imbalanced datasets where one class dominates.\n",
    "\n",
    "**Formula**:\n",
    "\\[\n",
    "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "## y_test --> [1 0 1]\n",
    "## y_pred-->[1 1 1]\n",
    "\n",
    "## (TruePositive+TruNegative)/ ()\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation**:\n",
    "- Accuracy is a good metric when the dataset is balanced (similar number of positive and negative samples).\n",
    "- For imbalanced datasets, other metrics like precision and recall should be prioritized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Precision**\n",
    "Precision tells us the proportion of positive predictions that were actually correct. It is useful when the cost of false positives is high (e.g., diagnosing a disease).\n",
    "\n",
    "**Formula**:\n",
    "\\[\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "\\]\n",
    "\n",
    "\n",
    "**Scenario**: You want to minimize false positives because they could result in predicting that a customer will buy when they actually won’t, leading to unnecessary marketing expenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision:\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation**:\n",
    "- Precision focuses on how many of the predicted \"Yes\" values were actually correct.\n",
    "- A high precision score means the model makes few false positives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Recall (Sensitivity or True Positive Rate)**\n",
    "Recall tells us the proportion of actual positives that were correctly identified. It is useful when missing a positive case is costly (e.g., missing out on identifying a fraud).\n",
    "\n",
    "**Formula**:\n",
    "\\[\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "\\]\n",
    "\n",
    "**Scenario**: You want to catch as many potential buyers as possible (minimizing false negatives), so recall becomes important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- Recall focuses on how many of the actual \"Yes\" customers the model correctly predicted.\n",
    "- A high recall score means fewer false negatives, so most actual \"Yes\" cases were captured by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5. F1 Score**\n",
    "The F1 score is the harmonic mean of precision and recall, combining both into a single metric. It is especially useful when you need a balance between precision and recall.\n",
    "\n",
    "**Formula**:\n",
    "\\[\n",
    "F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n",
    "\\]\n",
    "\n",
    "**Scenario**: In situations where both false positives and false negatives are important, you can use the F1 score to balance precision and recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- The F1 score provides a single metric that balances both precision and recall.\n",
    "- A higher F1 score means the model does well on both reducing false positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Cross Validation\n",
    "#### Cross-validation is a statistical method used to estimate the performance of a machine learning model. Instead of training the model on a single training set and evaluating it on a single test set, cross-validation splits the dataset into multiple \"folds\" and trains and tests the model multiple times.\n",
    "\n",
    "#### The idea behind cross-validation is to provide a better understanding of how the model performs on different subsets of data, giving a more reliable performance estimate.\n",
    "\n",
    "##### `Scenario`: You want to ensure that your model performs consistently across different subsets of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is Cross-Validation Important?\n",
    "\n",
    "##### `Reduces Overfitting`: It ensures the model doesn’t memorize the data but instead generalizes well to unseen data.\n",
    "##### `Better Model Evaluation`: Instead of evaluating the model on just one test set, cross-validation evaluates the model on several subsets of data, providing a more accurate estimate of its performance.\n",
    "##### `Handles Data Variability`: Cross-validation captures the variability in data, especially when data is scarce or imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of Cross-Validation\n",
    "\n",
    "1. K-Fold Cross-Validation\n",
    "In K-Fold Cross-Validation, the dataset is split into K equal parts (called folds). The model is trained on K-1 folds and tested on the remaining fold. This process is repeated K times, each time using a different fold as the test set. The performance of the model is then averaged over the K iterations.\n",
    "\n",
    "Scenario: You have a small dataset, and you want to ensure that the model gets trained and tested on every portion of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [nan  1.  1.  1.  1.]\n",
      "Average CV score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Codes\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "e:\\Codes\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "1 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\Codes\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"e:\\Codes\\.venv\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\Codes\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1301, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(1)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(log_reg, X_train_scaled, y_train, cv=5)\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Average CV score:\", np.mean(cv_scores))\n",
    "\n",
    "\n",
    "# Explanation:\n",
    "\n",
    "# cross_val_score helps evaluate the model's performance using multiple subsets of data. \n",
    "# You can average the scores to get a more reliable estimate of model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Stratified K-Fold Cross-Validation\n",
    "In Stratified K-Fold, the folds are made in such a way that each fold contains approximately the same proportion of samples from each class as the original dataset. This is especially useful when dealing with imbalanced datasets, where some classes have far fewer samples than others.\n",
    "\n",
    "Scenario: You are working with an imbalanced dataset (e.g., predicting customer churn, where most customers don’t churn), and you want to ensure that each fold contains a similar proportion of churn and non-churn customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified Cross-validation scores: [nan  1.  1.  1.  1.]\n",
      "Average CV score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Codes\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "e:\\Codes\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "1 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\Codes\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"e:\\Codes\\.venv\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\Codes\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1301, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(1)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# Use StratifiedKFold for cross-validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "cv_scores = cross_val_score(log_reg, X_train_scaled, y_train, cv=skf)\n",
    "print(\"Stratified Cross-validation scores:\", cv_scores)\n",
    "print(\"Average CV score:\", np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Clustering with K-Means\n",
    "#### K-Means is an unsupervised learning algorithm used to find groups in data.\n",
    "\n",
    "##### `Scenario`: You want to group customers into clusters based on their age and income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: [[0.81196581 0.73666667]\n",
      " [0.         0.        ]]\n",
      "Labels: [1 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Clustering customers\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(X_train_scaled)\n",
    "\n",
    "print(\"Cluster Centers:\", kmeans.cluster_centers_)\n",
    "print(\"Labels:\", kmeans.labels_)\n",
    "\n",
    "\n",
    "# Explanation:\n",
    "\n",
    "# K-Means groups data points into clusters based on similarity.\n",
    "# n_clusters=2 means we are looking for 2 groups in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
