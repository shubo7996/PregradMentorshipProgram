{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`Linear Algebra and Calculus`**\n",
    "\n",
    "### Here are the topics :\n",
    "\n",
    "#### 1. `Vectors and matrices`\n",
    "#### 2. `Linear equations`\n",
    "#### 3. `Eigenvalues and eigenvectors`\n",
    "#### 4. `Matrix transpose and inverse`\n",
    "#### 5. `Matrix multiplication and factorization`\n",
    "#### 6. `Linear transformations`\n",
    "#### 7. `Differentiation, integration, and gradient descent`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Vectors and Matrices**\n",
    "\n",
    "**Concept Explanation:**\n",
    "- **Vectors:** In linear algebra, vectors are elements of a vector space, which can be represented as an array of numbers. They can represent points, directions, or other quantities.\n",
    "- **Matrices:** A matrix is a rectangular array of numbers arranged in rows and columns. Matrices are used to represent linear transformations, systems of linear equations, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector:\n",
      "[2 3 5]\n",
      "\n",
      "Matrix:\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a vector\n",
    "vector = np.array([2, 3, 5]) #1D\n",
    "\n",
    "# Define a matrix\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9]]) #3x3\n",
    "\n",
    "print(\"Vector:\")\n",
    "print(vector)\n",
    "\n",
    "print(\"\\nMatrix:\")\n",
    "print(matrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Explanation:\n",
    "\n",
    "# We used numpy to define a vector and a matrix. The vector is a one-dimensional array, while the matrix is a two-dimensional array.\n",
    "# This basic representation is foundational for more complex operations in linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Linear Equations**\n",
    "\n",
    "**Concept Explanation:**\n",
    "- Linear equations are equations of the first degree, meaning they involve only linear terms (no exponents or powers). They are often represented in matrix form as \\(Ax = b\\), where \\(A\\) is a matrix of coefficients, \\(x\\) is a vector of unknowns, and \\(b\\) is a vector of constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 21. -12.]\n"
     ]
    }
   ],
   "source": [
    "#Ax=b\n",
    "\n",
    "#3x+5y=3 x2\n",
    "#2x+3y=6 x3\n",
    "\n",
    "# find the values of x and y\n",
    "\n",
    "# Coefficient matrix A\n",
    "# A = np.array([[3, 2, -1],\n",
    "#               [2, -2, 4],\n",
    "#               [-1, 0.5, -1]]) #3x3\n",
    "\n",
    "A=np.array([\n",
    "    [3,5],\n",
    "    [2,3]\n",
    "])\n",
    "\n",
    "\n",
    "# # Constants vector b\n",
    "# b = np.array([1, -2, 0])\n",
    "\n",
    "b=np.array([3,6])\n",
    "\n",
    "# # Solve for x\n",
    "x = np.linalg.solve(A, b)\n",
    "\n",
    "# print(\"Solution for x:\")\n",
    "print(x)\n",
    "\n",
    "# Explanation:\n",
    "\n",
    "# Here, we used numpy.linalg.solve() to solve the system of linear equations \n",
    "# ùê¥\n",
    "# ùë•\n",
    "# =\n",
    "# ùëè\n",
    "# Ax=b. This function finds the vector \n",
    "# ùë•\n",
    "# x that satisfies the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Eigenvalues and Eigenvectors**\n",
    "\n",
    "**Concept Explanation:**\n",
    "- **Eigenvalues and Eigenvectors:** For a given square matrix \\(A\\), if \\(v\\) is a non-zero vector, and \\(\\lambda\\) is a scalar such that \\(Av = \\lambda v\\), then \\(v\\) is an eigenvector of \\(A\\), and \\(\\lambda\\) is its corresponding eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Are Eigenvalues and Eigenvectors?\n",
    "\n",
    "Imagine you have a piece of paper with an arrow drawn on it. Now, you can stretch, rotate, or squash the paper, and that arrow will change in some way‚Äîits direction might change, and its length might change.\n",
    "\n",
    "Eigenvector: It's like a special arrow that, when you transform (stretch, rotate, etc.) the paper, it either stays pointing in the same direction or flips to point exactly opposite. It doesn‚Äôt twist off in some random direction. It‚Äôs like a steady arrow that holds its direction no matter how the paper is moved.\n",
    "\n",
    "Eigenvalue: This is the amount by which the eigenvector is stretched or squashed. If the eigenvalue is 2, it means the arrow (eigenvector) gets twice as long. If it's 0.5, the arrow gets half as long. If it‚Äôs a negative number, it means the arrow flips direction and stretches or squashes by that amount.\n",
    "\n",
    "\n",
    "\n",
    "Where Are They Useful in Machine Learning?\n",
    "Eigenvalues and eigenvectors are super important in machine learning, especially when dealing with data. Here's why:\n",
    "\n",
    "Principal Component Analysis (PCA):\n",
    "\n",
    "What It Does: PCA is a technique used to simplify complex data by reducing the number of dimensions (like compressing a big, detailed picture into a smaller one without losing too much important information).\n",
    "How It Uses Eigenvalues and Eigenvectors: It finds the eigenvectors (directions) where the data varies the most, and the eigenvalues tell you how much of the data's \"story\" is captured in each direction. By focusing on the directions with the biggest eigenvalues, you can simplify the data while still keeping the important parts.\n",
    "Understanding Patterns:\n",
    "\n",
    "In many machine learning algorithms, we need to find patterns or understand how data is structured. Eigenvalues and eigenvectors help us identify these patterns by highlighting the most important directions in the data.\n",
    "Facial Recognition:\n",
    "\n",
    "In facial recognition, eigenfaces (a term derived from eigenvectors) are used. The algorithm learns the important features of faces (like eyes, nose, etc.) by analyzing the data's eigenvectors and eigenvalues, helping it to recognize faces more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues:\n",
      "[3. 2.]\n",
      "\n",
      "Eigenvectors:\n",
      "[[0.89442719 0.70710678]\n",
      " [0.4472136  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "#a.v=lambda.v\n",
    "\n",
    "# Define a square matrix\n",
    "a = np.array([[4, -2],\n",
    "                   [1,  1]])\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(a)\n",
    "\n",
    "print(\"Eigenvalues:\")\n",
    "print(eigenvalues)\n",
    "\n",
    "print(\"\\nEigenvectors:\")\n",
    "print(eigenvectors)\n",
    "\n",
    "# **Explanation:**\n",
    "# - We used `numpy.linalg.eig()` to calculate the eigenvalues and eigenvectors of the matrix.\n",
    "# Eigenvalues are crucial in understanding the properties of matrices and linear transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **Matrix Transpose and Inverse**\n",
    "\n",
    "**Concept Explanation:**\n",
    "- **Transpose:** The transpose of a matrix is obtained by swapping its rows with its columns.\n",
    "- **Inverse:** The inverse of a matrix \\(A\\) is another matrix \\(A^{-1}\\) such that \\(AA^{-1} = I\\), where \\(I\\) is the identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpose of the matrix:\n",
      "[[ 4  1]\n",
      " [-2  1]]\n",
      "\n",
      "Inverse of the matrix:\n",
      "[[ 0.16666667  0.33333333]\n",
      " [-0.16666667  0.66666667]]\n"
     ]
    }
   ],
   "source": [
    "# Transpose of a matrix\n",
    "matrix = np.array([[4, -2],\n",
    "                   [1,  1]])\n",
    "\n",
    "transpose_matrix = matrix.T\n",
    "\n",
    "# Inverse of a matrix\n",
    "inverse_matrix = np.linalg.inv(matrix)\n",
    "\n",
    "print(\"Transpose of the matrix:\")\n",
    "print(transpose_matrix)\n",
    "\n",
    "print(\"\\nInverse of the matrix:\")\n",
    "print(inverse_matrix)\n",
    "\n",
    "\n",
    "# **Explanation:**\n",
    "# - The `.T` attribute is used to transpose the matrix, and `numpy.linalg.inv()` is used to calculate the inverse. \n",
    "# These operations are fundamental in solving systems of linear equations and in various applications like computer graphics and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **Matrix Multiplication and Factorization**\n",
    "\n",
    "**Concept Explanation:**\n",
    "- **Matrix Multiplication:** The product of two matrices is obtained by multiplying rows by columns.\n",
    "- **Factorization:** Matrix factorization (like LU decomposition) breaks down a matrix into simpler components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Downloading scipy-1.14.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in e:\\codes\\.venv\\lib\\site-packages (from scipy) (2.0.1)\n",
      "Downloading scipy-1.14.1-cp311-cp311-win_amd64.whl (44.8 MB)\n",
      "   ---------------------------------------- 0.0/44.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/44.8 MB 3.4 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 2.1/44.8 MB 5.9 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 5.0/44.8 MB 8.9 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 7.9/44.8 MB 10.1 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 10.5/44.8 MB 10.7 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 13.1/44.8 MB 11.1 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 15.7/44.8 MB 11.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 18.6/44.8 MB 11.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 21.2/44.8 MB 11.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 23.9/44.8 MB 11.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 26.7/44.8 MB 12.0 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 29.4/44.8 MB 12.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 32.2/44.8 MB 12.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 34.9/44.8 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 37.7/44.8 MB 12.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.4/44.8 MB 12.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 43.0/44.8 MB 12.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.6/44.8 MB 12.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.8/44.8 MB 12.1 MB/s eta 0:00:00\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LU Decomposition is like breaking down a complicated task into smaller, easier steps. Imagine you have a big LEGO structure. Instead of dealing with the whole thing at once, you first take it apart into smaller pieces (like walls and roofs) and then handle each piece separately. In math, LU Decomposition does something similar for matrices, which are big grids of numbers.\n",
    "\n",
    "Here‚Äôs how it works:\n",
    "\n",
    "L stands for \"Lower triangular matrix,\" which is like a set of simpler building blocks (like the base of your LEGO structure).\n",
    "U stands for \"Upper triangular matrix,\" which is another set of blocks (like the roof of your LEGO structure).\n",
    "So, LU Decomposition splits a complicated matrix into two simpler ones: L and U. This makes solving math problems, like finding solutions to systems of linear equations, much easier\n",
    "\n",
    "\n",
    "\n",
    "P√óA=L√óU\n",
    "\n",
    "Where:\n",
    "\n",
    "P is the permutation matrix (rearranging rows).\n",
    "A is the original matrix.\n",
    "L is the lower triangular matrix.\n",
    "U is the upper triangular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Product:\n",
      "[[1.00000000e+00 0.00000000e+00]\n",
      " [2.77555756e-17 1.00000000e+00]]\n",
      "\n",
      "LU Decomposition:\n",
      "P Matrix:\n",
      " [[1. 0.]\n",
      " [0. 1.]]\n",
      "L Matrix:\n",
      " [[1.   0.  ]\n",
      " [0.25 1.  ]]\n",
      "U Matrix:\n",
      " [[ 4.  -2. ]\n",
      " [ 0.   1.5]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "product = np.dot(matrix,inverse_matrix)\n",
    "\n",
    "print(\"Matrix Product:\")\n",
    "print(product)\n",
    "\n",
    "##LU Decomposition\n",
    "\n",
    "from scipy.linalg import lu\n",
    "P, L, U = lu(matrix)\n",
    "\n",
    "print(\"\\nLU Decomposition:\")\n",
    "print(\"P Matrix:\\n\", P)\n",
    "print(\"L Matrix:\\n\", L)\n",
    "print(\"U Matrix:\\n\", U)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# **Explanation:**\n",
    "# - We used `numpy.dot()` for matrix multiplication and `scipy.linalg.lu()` for LU decomposition, which is a factorization of a matrix into a product of a lower triangular matrix and an upper triangular matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. **Linear Transformations**\n",
    "\n",
    "**Concept Explanation:**\n",
    "- **Linear Transformations:** A linear transformation is a mapping between two vector spaces that preserves the operations of vector addition and scalar multiplication. These can be represented by matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Vector:\n",
      "[4 9]\n"
     ]
    }
   ],
   "source": [
    "# Linear transformation matrix\n",
    "transformation_matrix = np.array([[2, 0],\n",
    "                                  [0, 3]])\n",
    "\n",
    "##(2x2).(1x2)\n",
    "\n",
    "# Apply transformation to a vector\n",
    "transformed_vector = np.dot(transformation_matrix, vector[:2])\n",
    "\n",
    "print(\"Transformed Vector:\")\n",
    "print(transformed_vector)\n",
    "\n",
    "# **Explanation:**\n",
    "# - The transformation matrix scales the input vector by different factors along different axes. \n",
    "# This concept is widely used in computer graphics for scaling, rotation, and other transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`Calculus`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. **Differentiation, Integration, and Gradient Descent**\n",
    "\n",
    "**Concept Explanation:**\n",
    "- **Differentiation:** It is the process of finding the derivative of a function, which gives the rate of change of the function with respect to a variable.\n",
    "- **Integration:** The process of finding the integral of a function, representing the area under the curve of the function.\n",
    "- **Gradient Descent:** An optimization algorithm used to minimize functions by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sympy\n",
      "  Downloading sympy-1.13.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading sympy-1.13.2-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/6.2 MB 1.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.3/6.2 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.1/6.2 MB 2.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.1/6.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.7/6.2 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 4.4 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 5.8 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy\n",
      "Successfully installed mpmath-1.3.0 sympy-1.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function:\n",
      " 3      2        \n",
      "x  + 2‚ãÖx  + x + 1\n",
      "\n",
      "Derivative:\n",
      "   2          \n",
      "3‚ãÖx  + 4‚ãÖx + 1\n",
      "\n",
      "Integral:\n",
      " 4      3    2    \n",
      "x    2‚ãÖx    x     \n",
      "‚îÄ‚îÄ + ‚îÄ‚îÄ‚îÄ‚îÄ + ‚îÄ‚îÄ + x\n",
      "4     3     2     \n",
      "\n",
      "Minimum value found by Gradient Descent: -0.333333332962430\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define a symbol\n",
    "x = sp.symbols('x')\n",
    "\n",
    "# Define a function\n",
    "f = x**3 + 2*x**2 + x + 1 #f(x)=x^3+2x^2+x+1 \n",
    "\n",
    "# Differentiation\n",
    "f_prime = sp.diff(f, x) #f'(x)=x^3+2x^2+x+1= 3x^2+4x+1 \n",
    "\n",
    "# Integration\n",
    "f_integral = sp.integrate(f, x) \n",
    "\n",
    "print(\"Function:\")\n",
    "sp.pprint(f)\n",
    "\n",
    "print(\"\\nDerivative:\")\n",
    "sp.pprint(f_prime)\n",
    "\n",
    "print(\"\\nIntegral:\")\n",
    "sp.pprint(f_integral)\n",
    "\n",
    "# Gradient Descent example\n",
    "def gradient_descent(f_prime, start, learning_rate, iterations):\n",
    "    x_value = start\n",
    "    for _ in range(iterations):\n",
    "        x_value = x_value - learning_rate * f_prime.subs(x, x_value)\n",
    "    return x_value\n",
    "\n",
    "# Apply gradient descent\n",
    "min_value = gradient_descent(f_prime, start=0, learning_rate=0.01, iterations=1000)\n",
    "print(\"\\nMinimum value found by Gradient Descent:\", min_value)\n",
    "\n",
    "\n",
    "# <!-- **Explanation:**\n",
    "# - `sympy` is used for symbolic mathematics in Python. \n",
    "# We defined a function, computed its derivative and integral, \n",
    "# and then demonstrated gradient descent to find a local minimum of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
